{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b533d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import subplots\n",
    "from ISLP.models import (ModelSpec as MS, summarize, poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb04d38-4091-4765-9841-cbf5b95ad519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model (a) Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.239\n",
      "Model:                            OLS   Adj. R-squared:                  0.234\n",
      "Method:                 Least Squares   F-statistic:                     41.52\n",
      "Date:                Thu, 31 Oct 2024   Prob (F-statistic):           2.39e-23\n",
      "Time:                        21:39:58   Log-Likelihood:                -927.66\n",
      "No. Observations:                 400   AIC:                             1863.\n",
      "Df Residuals:                     396   BIC:                             1879.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept       13.0435      0.651     20.036      0.000      11.764      14.323\n",
      "Urban[T.Yes]    -0.0219      0.272     -0.081      0.936      -0.556       0.512\n",
      "US[T.Yes]        1.2006      0.259      4.635      0.000       0.691       1.710\n",
      "Price           -0.0545      0.005    -10.389      0.000      -0.065      -0.044\n",
      "==============================================================================\n",
      "Omnibus:                        0.676   Durbin-Watson:                   1.912\n",
      "Prob(Omnibus):                  0.713   Jarque-Bera (JB):                0.758\n",
      "Skew:                           0.093   Prob(JB):                        0.684\n",
      "Kurtosis:                       2.897   Cond. No.                         628.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "Significant predictors in model (a):\n",
      "Intercept    3.626602e-62\n",
      "US[T.Yes]    4.860245e-06\n",
      "Price        1.609917e-22\n",
      "dtype: float64\n",
      "Model (e) Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.198\n",
      "Model:                            OLS   Adj. R-squared:                  0.196\n",
      "Method:                 Least Squares   F-statistic:                     98.25\n",
      "Date:                Thu, 31 Oct 2024   Prob (F-statistic):           7.62e-21\n",
      "Time:                        21:39:58   Log-Likelihood:                -938.23\n",
      "No. Observations:                 400   AIC:                             1880.\n",
      "Df Residuals:                     398   BIC:                             1888.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     13.6419      0.633     21.558      0.000      12.398      14.886\n",
      "Price         -0.0531      0.005     -9.912      0.000      -0.064      -0.043\n",
      "==============================================================================\n",
      "Omnibus:                        2.537   Durbin-Watson:                   1.892\n",
      "Prob(Omnibus):                  0.281   Jarque-Bera (JB):                2.611\n",
      "Skew:                           0.175   Prob(JB):                        0.271\n",
      "Kurtosis:                       2.816   Cond. No.                         591.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "R^2 for model (a): 0.23927539218405525\n",
      "R^2 for model (e): 0.19798115021119478\n",
      "Interpretation: Larger R^2 does not necessarily mean a better model due to model complexity considerations.\n",
      "95% Confidence Intervals for Model (e) Coefficients:\n",
      "                   0          1\n",
      "Intercept  12.397844  14.885987\n",
      "Price      -0.063600  -0.042547\n",
      "Model (h) with Interaction Effects Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  Sales   R-squared:                       0.241\n",
      "Model:                            OLS   Adj. R-squared:                  0.231\n",
      "Method:                 Least Squares   F-statistic:                     25.01\n",
      "Date:                Thu, 31 Oct 2024   Prob (F-statistic):           6.61e-22\n",
      "Time:                        21:39:58   Log-Likelihood:                -927.22\n",
      "No. Observations:                 400   AIC:                             1866.\n",
      "Df Residuals:                     394   BIC:                             1890.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "======================================================================================\n",
      "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------\n",
      "Intercept             13.8557      1.349     10.274      0.000      11.204      16.507\n",
      "US[T.Yes]              1.3372      1.255      1.066      0.287      -1.130       3.805\n",
      "Urban[T.Yes]          -1.2652      1.370     -0.923      0.356      -3.959       1.429\n",
      "Price                 -0.0616      0.012     -5.323      0.000      -0.084      -0.039\n",
      "Price:US[T.Yes]       -0.0012      0.011     -0.116      0.908      -0.022       0.020\n",
      "Price:Urban[T.Yes]     0.0108      0.012      0.926      0.355      -0.012       0.034\n",
      "==============================================================================\n",
      "Omnibus:                        0.635   Durbin-Watson:                   1.914\n",
      "Prob(Omnibus):                  0.728   Jarque-Bera (JB):                0.701\n",
      "Skew:                           0.093   Prob(JB):                        0.704\n",
      "Kurtosis:                       2.914   Cond. No.                     2.51e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.51e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Load necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from ISLP import load_data\n",
    "data = load_data('Carseats')\n",
    "\n",
    "# Display dataset's first few rows\n",
    "data.head()\n",
    "\n",
    "# Part (a): Fit a multiple regression model to predict Sales using Price, Urban, and US\n",
    "# Define model\n",
    "model_a = smf.ols('Sales ~ Price + Urban + US', data=data).fit()\n",
    "\n",
    "# Display summary of model\n",
    "print(\"Model (a) Summary:\")\n",
    "print(model_a.summary())\n",
    "\n",
    "# Part (b): Interpret each coefficient in the model\n",
    "# This is discussed in the interpretation section\n",
    "\n",
    "# Part (c): Model equation with qualitative variables handled appropriately\n",
    "# 'Urban' and 'US' are qualitative and are handled via automatic encoding by statsmodels\n",
    "\n",
    "# Part (d): Identify predictors with significant p-values\n",
    "significant_predictors = model_a.pvalues[model_a.pvalues < 0.05]\n",
    "print(\"Significant predictors in model (a):\")\n",
    "print(significant_predictors)\n",
    "\n",
    "# Part (e): Fit a smaller model using only significant predictors\n",
    "# Based on part (d), only include significant predictors\n",
    "# Adjust model formula based on (d) results\n",
    "model_e = smf.ols('Sales ~ Price', data=data).fit()\n",
    "\n",
    "# Display summary of the smaller model\n",
    "print(\"Model (e) Summary:\")\n",
    "print(model_e.summary())\n",
    "\n",
    "# Part (f): Compare R^2 values for models (a) and (e)\n",
    "r2_a = model_a.rsquared\n",
    "r2_e = model_e.rsquared\n",
    "print(\"R^2 for model (a):\", r2_a)\n",
    "print(\"R^2 for model (e):\", r2_e)\n",
    "print(\"Interpretation: Larger R^2 does not necessarily mean a better model due to model complexity considerations.\")\n",
    "\n",
    "# Part (g): Obtain 95% confidence intervals for the coefficients in the model (e)\n",
    "conf_intervals = model_e.conf_int(alpha=0.05)\n",
    "print(\"95% Confidence Intervals for Model (e) Coefficients:\")\n",
    "print(conf_intervals)\n",
    "\n",
    "# Part (h): Fit model with interaction terms\n",
    "# Including interaction between Price and other categorical variables if relevant\n",
    "model_h = smf.ols('Sales ~ Price * US + Price * Urban', data=data).fit()\n",
    "\n",
    "# Display summary of the interaction model\n",
    "print(\"Model (h) with Interaction Effects Summary:\")\n",
    "print(model_h.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2d684-65c0-42e5-a39c-067d8c9a6b56",
   "metadata": {},
   "source": [
    "### (a) Simulation in Python\n",
    "\n",
    "To answer the question, e'll generate `n = 500` data points from a simple linear regression model $ y = \\beta x + \\epsilon $ without an intercept. Here’s a step-by-step plan for the simulation:\n",
    "\n",
    "1. **Data Generation**:\n",
    "   - Set $$ x $$ to be generated from a normal distribution $ x \\sim N(0, 1) $.\n",
    "   - Set $ \\epsilon \\sim N(0, \\sigma^2) $, where $ \\sigma $ is the standard deviation of the noise.\n",
    "   - Set $ \\beta $ to a fixed value, such as 2.\n",
    "\n",
    "2. **Least Squares Regression of $ x $ on $ y $**:\n",
    "   - For each iteration, generate new data and perform the regression of $ x $ onto $ y $.\n",
    "   - Store the coefficient estimate for each regression.\n",
    "\n",
    "3. **Repeat the Simulation 100 Times**:\n",
    "   - Compute the mean of the 100 estimates and compare it to $ 1/\\beta $.\n",
    "\n",
    "### Interpretation of Results:\n",
    "- If the mean of the estimates of the regression coefficient for $ x $ on $ y $ is close to $ 1/\\beta $, this suggests that the estimate is a good approximation of $ 1/\\beta $.\n",
    "- If the estimate consistently diverges from $ 1/\\beta $, then the regression of $ x $ onto $ y $ does not provide a reliable estimate of $ 1/\\beta $.\n",
    "\n",
    "The above simulation provides insight that the coefficient of $ x $ regressed on $ y $ is not a reliable estimate of $ 1/\\beta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c5ae2f-3b2b-466f-91d6-117c8aabbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of coefficient estimates for x on y: 0.40096625668646546\n",
      "Expected value 1/beta: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parameters\n",
    "n = 500       # Number of data points\n",
    "beta = 2.0    # True coefficient\n",
    "sigma = 1.0   # Standard deviation of noise\n",
    "num_simulations = 100\n",
    "\n",
    "# Storage for coefficient estimates\n",
    "coef_estimates = []\n",
    "\n",
    "# Simulation loop\n",
    "for _ in range(num_simulations):\n",
    "    # Generate x and noise epsilon\n",
    "    x = np.random.normal(0, 1, n)\n",
    "    epsilon = np.random.normal(0, sigma, n)\n",
    "    \n",
    "    # Generate y according to the model without intercept\n",
    "    y = beta * x + epsilon\n",
    "    \n",
    "    # Regress x on y without intercept\n",
    "    model = sm.OLS(x, y)\n",
    "    results = model.fit()\n",
    "    coef_estimates.append(results.params[0])\n",
    "\n",
    "# Calculate the mean of the estimates and compare to 1/beta\n",
    "mean_coef_estimate = np.mean(coef_estimates)\n",
    "expected_inverse_beta = 1 / beta\n",
    "\n",
    "print(\"Mean of coefficient estimates for x on y:\", mean_coef_estimate)\n",
    "print(\"Expected value 1/beta:\", expected_inverse_beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95c173-339e-426f-954e-82a2185d5584",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### (b) Proof of Consistency for the Estimate of $ 1/\\beta $\n",
    "\n",
    "We aim to prove whether the least squares estimator $ \\hat{\\alpha} $ for the regression of $ x $ on $ y $ is a consistent estimator of $ 1/\\beta $.\n",
    "\n",
    "1. **Define the Regression Estimator**:\n",
    "   In the regression of y on x, we have:\n",
    "   $$\n",
    "   \\hat{\\beta} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}$$\n",
    "   \n",
    "   When we regress $ x $ on $ y $, the least squares estimator $ \\hat{\\alpha} $ is:\n",
    "   $$\n",
    "   \\hat{\\alpha} = \\frac{\\sum_{i=1}^n y_i x_i}{\\sum_{i=1}^n y_i^2}\n",
    "   $$\n",
    "\n",
    "2. **Substitute $ y_i = \\beta x_i + \\epsilon_i $**:\n",
    "   Substituting $ y_i $ from the model into $ \\hat{\\alpha} $, we get:\n",
    "   $$\n",
    "   \\hat{\\alpha} = \\frac{\\sum_{i=1}^n (\\beta x_i + \\epsilon_i) x_i}{\\sum_{i=1}^n (\\beta x_i + \\epsilon_i)^2}\n",
    "   = \\frac{\\beta \\sum_{i=1}^n x_i^2 + \\sum_{i=1}^n x_i \\epsilon_i}{\\beta^2 \\sum_{i=1}^n x_i^2 + 2\\beta \\sum_{i=1}^n x_i \\epsilon_i + \\sum_{i=1}^n \\epsilon_i^2}\n",
    "   $$\n",
    "\n",
    "3. **Apply the Law of Large Numbers**:\n",
    "   As $ n \\to \\infty $:\n",
    "   - By the Law of Large Numbers, $ \\frac{1}{n} \\sum_{i=1}^n x_i^2 \\to E[x^2] $ and $ \\frac{1}{n} \\sum_{i=1}^n \\epsilon_i^2 \\to E[\\epsilon^2] $.\n",
    "   - The cross terms $ \\frac{1}{n} \\sum_{i=1}^n x_i \\epsilon_i \\to 0 $ by independence of $ x_i $ and $ \\epsilon_i $.\n",
    "\n",
    "   Therefore:\n",
    "   $$\n",
    "   \\hat{\\alpha} \\approx \\frac{\\beta E[x^2]}{\\beta^2 E[x^2] + E[\\epsilon^2]}\n",
    "   $$\n",
    "   But:\n",
    "   $$E[\\epsilon^2] = 0$$\n",
    "    so\n",
    "\n",
    "   $$\\hat{\\alpha} \\approx \\frac{\\beta E[x^2]}{\\beta^2 E[x^2]}$$\n",
    "5. **Conclusion**:\n",
    "   Since $ \\frac{\\beta E[x^2]}{\\beta E[x^2] + E[\\epsilon^2]} = \\frac{1}{\\beta} $, $ \\hat{\\alpha} $ is **a consistent estimator** of $ 1/\\beta $.\n",
    "\n",
    "This demonstrates that it does  converge to $ 1/\\beta $ as $ n \\to \\infty $, meaning regressing $ x $ onto $ y $ does yield a consistent estimate of $ 1/\\beta $ in this setup."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
